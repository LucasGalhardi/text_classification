
### Análise do problema e experiência prévia
Ao ler a descrição do problema já ficou claro que se tratava de uma classificação de texto padrão, com a exceção do aspecto multi-label. No meu trabalho de mestrado eu trabalhei justamente com isso (classificação de texto) e a minha esposa também (e ela ainda com multi-target!). Então já sabia mais ou menos por onde ir.

### Análise do problema e escolha da abordagem
Comecei analisando o dataset manualmente, observando a língua (português), tamanho das frases e sua "complexidade". Me pareceu que os textos eram relativamente bem simples e pequenos, o que me fez decidir ir por uma abordagem mais simples também. Ou seja, resolvei utilizar uma modelagem bag-of-words/ngramas (com TF-IDF) e não utilizar abordagens mais modernas e avançadas, para dados mais complexos (como word embeddings, ElMO, BERT (e suas variantes) e Transformers). Além da simplicidade dos dados, levei em conta também os resultados que obtive no meu mestrado: apesar de utilizar técnicas mais avançadas e algoritmos diferenciados, geralmente o modelo de ngramas levava a melhor. Além, é claro, do tempo disponível, pois caso houvesse mais tempo poderia investigar diferentes abordagens para extração das features.

Depois analisei as labels, percebendo que em sua maioria as frases acabavam sendo classificadas em apenas 1 categoria, mas que havia 9 combinações de 2 classes. Devido à isso, já pensei que a técnica do PowerSet poderia se aplicar melhor, visto que o modelo poderia se especializar em aprender as combinações mais recorrentes das labels como se fosse uma nova classe.

### Pré-processamento
Em relação ao pré-processamento, utilizei e adaptei um módulo que eu já tinha (justamente do projeto do mestrado). Passei algumas técnicas padrões como lowercase, remoção de acentos, tokenização, stemming e remoção de stopwords/tokens muito pequenos. Tirei algumas funções do módulo e criei algumas novas para tokenizar algumas informações específicas que considerei serem relevantes para o problema em questão. Em alguns momentos não consegui utilizar as expressões regulares mais adequadas e acabei deixando padrões mais simples. Testei com algumas amostras e funcionou razoável (embora eu percebi que não funcionou para todos, o que exigiria expressões regulares bem mais elaboradas). De acordo com as categorias, percebi algumas informações que poderiam ser relevantes: datas (para órgãos públicos), parcelas (para varejo), valores monetários (para finanças), porcentagens (para varejo) e números no geral (varejo e finanças).

### Front-end
Depois de testar o modelo com o mais básico funcionando, parti para a implementação do front-end. Criei uma aplicação React simples, utilizando apenas 1 formulário bootstrap e escrevendo como utilizar a aplicação. Como não tenho muita habilidade com o layout escolhi algo simples: utilizar botões coloridos para indicar a resposta. A frase é inserida, enviada por requisição POST e é retornado uma lista de 0s e 1s, que é convertida para as variantes 'light' e 'success', exibindo em verde qual classe (ou quais classes) aquela frase pertence.

### Back-end + organização com o modelo
No back-end foi criada 2 views: uma que realiza o treinamento e uma que realiza a predição. Para organizar melhor o código, o modelo que foi criado e testado no "evaluation.py" foi "transferido" para uma classe DataManipulation que encapsula o acesso e provê um objeto único tanto para a etapa de treinamento e predição, já guardando junto tanto o modelo quanto o binarizer e o vectorizer. Estruturando dessa forma, a lógica das views está separada da lógica do modelo (DataManipulation). Por simplicidade, os experimentos (do vectorizer, pré-processamento, parâmetros e etc) foram feitos no script "evaluation.py" e "migrados" para o DataManipulation (DM). A view de treinamento apenas cria o objeto DM, chama o treinamento e guarda o resultado em disco. Por sua vez, a view de predição recebe uma frase nova, resgata o objeto DM do disco, chama o predict na nova frase e retorna um JSON com a resposta em formato de lista de 0s e 1s.

### Experimentos para definição do modelo final
Por fim, eu voltei no modelo e fiz novos experimentos, variando a abordagem multi-label (Problem Adaption e Problem Transformation: Binary Relevance x Label PowerSet) e os algoritmos (KNN, Random Forest, Bagging, SVM, Naive Bayes e Gradient Boosting). Desses, o melhor micro f1-score obtido foi o LinearSVC com a estratégia do PowerSet com 0.751.